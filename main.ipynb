{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "f59f9892-29bf-4af3-983e-0e34084b1174",
      "metadata": {
        "id": "f59f9892-29bf-4af3-983e-0e34084b1174"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1e976f4b",
      "metadata": {},
      "source": [
        "## About Data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8bcf47a4",
      "metadata": {},
      "source": [
        "Twitter has become an important communication channel in times of emergency.\n",
        "The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n",
        "\n",
        "But, it’s not always clear whether a person’s words are actually announcing a disaster. Take this example:\n",
        "![image.png](assets/tweet_screenshot.png)\n",
        "\n",
        "The author explicitly uses the word “ABLAZE” but means it metaphorically. This is clear to a human right away, especially with the visual aid. But it’s less clear to a machine.\n",
        "\n",
        "-------\n",
        "Columns: \n",
        "\n",
        "id - a unique identifier for each tweet\n",
        "\n",
        "text - the text of the tweet\n",
        "\n",
        "location - the location the tweet was sent from (may be blank)\n",
        "\n",
        "keyword - a particular keyword from the tweet (may be blank)\n",
        "\n",
        "target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7cb3c716",
      "metadata": {},
      "source": [
        "## Data Prepration"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4090f26d",
      "metadata": {},
      "source": [
        "### Reading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "c993f2ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(r'data\\train.csv')\n",
        "test = pd.read_csv(r'data\\test.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2c29298f",
      "metadata": {},
      "source": [
        "### Investigating the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "cb2d2f63-ec4c-472f-8c43-7437bb0c02c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cb2d2f63-ec4c-472f-8c43-7437bb0c02c6",
        "outputId": "deafc6dd-14ee-4ecf-85b6-548a64443809"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "____________shape_____________\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(7613, 5)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_____________head_____________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_f3f06_row0_col0, #T_f3f06_row0_col4, #T_f3f06_row1_col4, #T_f3f06_row2_col4, #T_f3f06_row3_col4, #T_f3f06_row4_col4 {\n",
              "  background-color: #f7fbff;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f3f06_row1_col0 {\n",
              "  background-color: #6aaed6;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_f3f06_row2_col0 {\n",
              "  background-color: #3787c0;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_f3f06_row3_col0 {\n",
              "  background-color: #105ba4;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_f3f06_row4_col0 {\n",
              "  background-color: #08306b;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_f3f06\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_f3f06_level0_col0\" class=\"col_heading level0 col0\" >id</th>\n",
              "      <th id=\"T_f3f06_level0_col1\" class=\"col_heading level0 col1\" >keyword</th>\n",
              "      <th id=\"T_f3f06_level0_col2\" class=\"col_heading level0 col2\" >location</th>\n",
              "      <th id=\"T_f3f06_level0_col3\" class=\"col_heading level0 col3\" >text</th>\n",
              "      <th id=\"T_f3f06_level0_col4\" class=\"col_heading level0 col4\" >target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_f3f06_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_f3f06_row0_col0\" class=\"data row0 col0\" >1</td>\n",
              "      <td id=\"T_f3f06_row0_col1\" class=\"data row0 col1\" >nan</td>\n",
              "      <td id=\"T_f3f06_row0_col2\" class=\"data row0 col2\" >nan</td>\n",
              "      <td id=\"T_f3f06_row0_col3\" class=\"data row0 col3\" >Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
              "      <td id=\"T_f3f06_row0_col4\" class=\"data row0 col4\" >1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f3f06_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_f3f06_row1_col0\" class=\"data row1 col0\" >4</td>\n",
              "      <td id=\"T_f3f06_row1_col1\" class=\"data row1 col1\" >nan</td>\n",
              "      <td id=\"T_f3f06_row1_col2\" class=\"data row1 col2\" >nan</td>\n",
              "      <td id=\"T_f3f06_row1_col3\" class=\"data row1 col3\" >Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td id=\"T_f3f06_row1_col4\" class=\"data row1 col4\" >1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f3f06_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_f3f06_row2_col0\" class=\"data row2 col0\" >5</td>\n",
              "      <td id=\"T_f3f06_row2_col1\" class=\"data row2 col1\" >nan</td>\n",
              "      <td id=\"T_f3f06_row2_col2\" class=\"data row2 col2\" >nan</td>\n",
              "      <td id=\"T_f3f06_row2_col3\" class=\"data row2 col3\" >All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
              "      <td id=\"T_f3f06_row2_col4\" class=\"data row2 col4\" >1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f3f06_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_f3f06_row3_col0\" class=\"data row3 col0\" >6</td>\n",
              "      <td id=\"T_f3f06_row3_col1\" class=\"data row3 col1\" >nan</td>\n",
              "      <td id=\"T_f3f06_row3_col2\" class=\"data row3 col2\" >nan</td>\n",
              "      <td id=\"T_f3f06_row3_col3\" class=\"data row3 col3\" >13,000 people receive #wildfires evacuation orders in California </td>\n",
              "      <td id=\"T_f3f06_row3_col4\" class=\"data row3 col4\" >1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f3f06_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_f3f06_row4_col0\" class=\"data row4 col0\" >7</td>\n",
              "      <td id=\"T_f3f06_row4_col1\" class=\"data row4 col1\" >nan</td>\n",
              "      <td id=\"T_f3f06_row4_col2\" class=\"data row4 col2\" >nan</td>\n",
              "      <td id=\"T_f3f06_row4_col3\" class=\"data row4 col3\" >Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school </td>\n",
              "      <td id=\"T_f3f06_row4_col4\" class=\"data row4 col4\" >1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x249e49c4400>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_____________tail_____________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_ffa21_row0_col0, #T_ffa21_row0_col4, #T_ffa21_row1_col4, #T_ffa21_row2_col4, #T_ffa21_row3_col4, #T_ffa21_row4_col4 {\n",
              "  background-color: #f7fbff;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_ffa21_row1_col0 {\n",
              "  background-color: #c6dbef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_ffa21_row2_col0 {\n",
              "  background-color: #6aaed6;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_ffa21_row3_col0 {\n",
              "  background-color: #2070b4;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_ffa21_row4_col0 {\n",
              "  background-color: #08306b;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_ffa21\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_ffa21_level0_col0\" class=\"col_heading level0 col0\" >id</th>\n",
              "      <th id=\"T_ffa21_level0_col1\" class=\"col_heading level0 col1\" >keyword</th>\n",
              "      <th id=\"T_ffa21_level0_col2\" class=\"col_heading level0 col2\" >location</th>\n",
              "      <th id=\"T_ffa21_level0_col3\" class=\"col_heading level0 col3\" >text</th>\n",
              "      <th id=\"T_ffa21_level0_col4\" class=\"col_heading level0 col4\" >target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_ffa21_level0_row0\" class=\"row_heading level0 row0\" >7608</th>\n",
              "      <td id=\"T_ffa21_row0_col0\" class=\"data row0 col0\" >10869</td>\n",
              "      <td id=\"T_ffa21_row0_col1\" class=\"data row0 col1\" >nan</td>\n",
              "      <td id=\"T_ffa21_row0_col2\" class=\"data row0 col2\" >nan</td>\n",
              "      <td id=\"T_ffa21_row0_col3\" class=\"data row0 col3\" >Two giant cranes holding a bridge collapse into nearby homes http://t.co/STfMbbZFB5</td>\n",
              "      <td id=\"T_ffa21_row0_col4\" class=\"data row0 col4\" >1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ffa21_level0_row1\" class=\"row_heading level0 row1\" >7609</th>\n",
              "      <td id=\"T_ffa21_row1_col0\" class=\"data row1 col0\" >10870</td>\n",
              "      <td id=\"T_ffa21_row1_col1\" class=\"data row1 col1\" >nan</td>\n",
              "      <td id=\"T_ffa21_row1_col2\" class=\"data row1 col2\" >nan</td>\n",
              "      <td id=\"T_ffa21_row1_col3\" class=\"data row1 col3\" >@aria_ahrary @TheTawniest The out of control wild fires in California even in the Northern part of the state. Very troubling.</td>\n",
              "      <td id=\"T_ffa21_row1_col4\" class=\"data row1 col4\" >1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ffa21_level0_row2\" class=\"row_heading level0 row2\" >7610</th>\n",
              "      <td id=\"T_ffa21_row2_col0\" class=\"data row2 col0\" >10871</td>\n",
              "      <td id=\"T_ffa21_row2_col1\" class=\"data row2 col1\" >nan</td>\n",
              "      <td id=\"T_ffa21_row2_col2\" class=\"data row2 col2\" >nan</td>\n",
              "      <td id=\"T_ffa21_row2_col3\" class=\"data row2 col3\" >M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http://t.co/zDtoyd8EbJ</td>\n",
              "      <td id=\"T_ffa21_row2_col4\" class=\"data row2 col4\" >1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ffa21_level0_row3\" class=\"row_heading level0 row3\" >7611</th>\n",
              "      <td id=\"T_ffa21_row3_col0\" class=\"data row3 col0\" >10872</td>\n",
              "      <td id=\"T_ffa21_row3_col1\" class=\"data row3 col1\" >nan</td>\n",
              "      <td id=\"T_ffa21_row3_col2\" class=\"data row3 col2\" >nan</td>\n",
              "      <td id=\"T_ffa21_row3_col3\" class=\"data row3 col3\" >Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffered serious non-life threatening injuries.</td>\n",
              "      <td id=\"T_ffa21_row3_col4\" class=\"data row3 col4\" >1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ffa21_level0_row4\" class=\"row_heading level0 row4\" >7612</th>\n",
              "      <td id=\"T_ffa21_row4_col0\" class=\"data row4 col0\" >10873</td>\n",
              "      <td id=\"T_ffa21_row4_col1\" class=\"data row4 col1\" >nan</td>\n",
              "      <td id=\"T_ffa21_row4_col2\" class=\"data row4 col2\" >nan</td>\n",
              "      <td id=\"T_ffa21_row4_col3\" class=\"data row4 col3\" >The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d</td>\n",
              "      <td id=\"T_ffa21_row4_col4\" class=\"data row4 col4\" >1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x249e4931880>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_____________info_____________\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7613 entries, 0 to 7612\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        7613 non-null   int64 \n",
            " 1   keyword   7552 non-null   object\n",
            " 2   location  5080 non-null   object\n",
            " 3   text      7613 non-null   object\n",
            " 4   target    7613 non-null   int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 297.5+ KB\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_____describe_continuous______\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_de678_row0_col0, #T_de678_row1_col0, #T_de678_row1_col1, #T_de678_row1_col2, #T_de678_row1_col3, #T_de678_row1_col4, #T_de678_row1_col5, #T_de678_row1_col6, #T_de678_row1_col7 {\n",
              "  background-color: #f7fbff;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_de678_row0_col1, #T_de678_row0_col2, #T_de678_row0_col3, #T_de678_row0_col4, #T_de678_row0_col5, #T_de678_row0_col6, #T_de678_row0_col7 {\n",
              "  background-color: #08306b;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_de678\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_de678_level0_col0\" class=\"col_heading level0 col0\" >count</th>\n",
              "      <th id=\"T_de678_level0_col1\" class=\"col_heading level0 col1\" >mean</th>\n",
              "      <th id=\"T_de678_level0_col2\" class=\"col_heading level0 col2\" >std</th>\n",
              "      <th id=\"T_de678_level0_col3\" class=\"col_heading level0 col3\" >min</th>\n",
              "      <th id=\"T_de678_level0_col4\" class=\"col_heading level0 col4\" >25%</th>\n",
              "      <th id=\"T_de678_level0_col5\" class=\"col_heading level0 col5\" >50%</th>\n",
              "      <th id=\"T_de678_level0_col6\" class=\"col_heading level0 col6\" >75%</th>\n",
              "      <th id=\"T_de678_level0_col7\" class=\"col_heading level0 col7\" >max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_de678_level0_row0\" class=\"row_heading level0 row0\" >id</th>\n",
              "      <td id=\"T_de678_row0_col0\" class=\"data row0 col0\" >7613.000000</td>\n",
              "      <td id=\"T_de678_row0_col1\" class=\"data row0 col1\" >5441.934848</td>\n",
              "      <td id=\"T_de678_row0_col2\" class=\"data row0 col2\" >3137.116090</td>\n",
              "      <td id=\"T_de678_row0_col3\" class=\"data row0 col3\" >1.000000</td>\n",
              "      <td id=\"T_de678_row0_col4\" class=\"data row0 col4\" >2734.000000</td>\n",
              "      <td id=\"T_de678_row0_col5\" class=\"data row0 col5\" >5408.000000</td>\n",
              "      <td id=\"T_de678_row0_col6\" class=\"data row0 col6\" >8146.000000</td>\n",
              "      <td id=\"T_de678_row0_col7\" class=\"data row0 col7\" >10873.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_de678_level0_row1\" class=\"row_heading level0 row1\" >target</th>\n",
              "      <td id=\"T_de678_row1_col0\" class=\"data row1 col0\" >7613.000000</td>\n",
              "      <td id=\"T_de678_row1_col1\" class=\"data row1 col1\" >0.429660</td>\n",
              "      <td id=\"T_de678_row1_col2\" class=\"data row1 col2\" >0.495060</td>\n",
              "      <td id=\"T_de678_row1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
              "      <td id=\"T_de678_row1_col4\" class=\"data row1 col4\" >0.000000</td>\n",
              "      <td id=\"T_de678_row1_col5\" class=\"data row1 col5\" >0.000000</td>\n",
              "      <td id=\"T_de678_row1_col6\" class=\"data row1 col6\" >1.000000</td>\n",
              "      <td id=\"T_de678_row1_col7\" class=\"data row1 col7\" >1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x249e49c4400>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_____describe_categorical_____\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_a22ff\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_a22ff_level0_col0\" class=\"col_heading level0 col0\" >count</th>\n",
              "      <th id=\"T_a22ff_level0_col1\" class=\"col_heading level0 col1\" >unique</th>\n",
              "      <th id=\"T_a22ff_level0_col2\" class=\"col_heading level0 col2\" >top</th>\n",
              "      <th id=\"T_a22ff_level0_col3\" class=\"col_heading level0 col3\" >freq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_a22ff_level0_row0\" class=\"row_heading level0 row0\" >keyword</th>\n",
              "      <td id=\"T_a22ff_row0_col0\" class=\"data row0 col0\" >7552</td>\n",
              "      <td id=\"T_a22ff_row0_col1\" class=\"data row0 col1\" >221</td>\n",
              "      <td id=\"T_a22ff_row0_col2\" class=\"data row0 col2\" >fatalities</td>\n",
              "      <td id=\"T_a22ff_row0_col3\" class=\"data row0 col3\" >45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a22ff_level0_row1\" class=\"row_heading level0 row1\" >location</th>\n",
              "      <td id=\"T_a22ff_row1_col0\" class=\"data row1 col0\" >5080</td>\n",
              "      <td id=\"T_a22ff_row1_col1\" class=\"data row1 col1\" >3341</td>\n",
              "      <td id=\"T_a22ff_row1_col2\" class=\"data row1 col2\" >USA</td>\n",
              "      <td id=\"T_a22ff_row1_col3\" class=\"data row1 col3\" >104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a22ff_level0_row2\" class=\"row_heading level0 row2\" >text</th>\n",
              "      <td id=\"T_a22ff_row2_col0\" class=\"data row2 col0\" >7613</td>\n",
              "      <td id=\"T_a22ff_row2_col1\" class=\"data row2 col1\" >7503</td>\n",
              "      <td id=\"T_a22ff_row2_col2\" class=\"data row2 col2\" >11-Year-Old Boy Charged With Manslaughter of Toddler: Report: An 11-year-old boy has been charged with manslaughter over the fatal sh...</td>\n",
              "      <td id=\"T_a22ff_row2_col3\" class=\"data row2 col3\" >10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x249e49c4400>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_____null_values_percent______\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "location    33.272035\n",
              "keyword      0.801261\n",
              "id           0.000000\n",
              "text         0.000000\n",
              "target       0.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def background_color(value):\n",
        "    if isinstance(value, str):\n",
        "        return 'background-color: #a6c0ed'\n",
        "    return ''\n",
        "\n",
        "def show_df(df_train):\n",
        "    print('shape'.center(30,'_'))\n",
        "    display(df_train.shape)\n",
        "\n",
        "    print('head'.center(30,'_'))\n",
        "    display(df_train.head().style.background_gradient(cmap='Blues'))\n",
        "\n",
        "    print('tail'.center(30,'_'))\n",
        "    display(df_train.tail().style.background_gradient(cmap='Blues'))\n",
        "\n",
        "    print('info'.center(30,'_')+'\\n')\n",
        "    display(df_train.info())\n",
        "\n",
        "    print('describe_continuous'.center(30,'_'))\n",
        "    display(df_train.describe().T.style.background_gradient(cmap = 'Blues'))\n",
        "\n",
        "    print('describe_categorical'.center(30,'_'))\n",
        "    display(df_train.describe(include='object').T.style.background_gradient(cmap='Blues'))\n",
        "\n",
        "    print('null_values_percent'.center(30,'_'))\n",
        "    display((df_train.isna().sum() / len(df_train) * 100).sort_values(ascending=False))\n",
        "show_df(df_train)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7a8e9dce",
      "metadata": {},
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9e2c341f",
      "metadata": {},
      "source": [
        "#### Removing URL's from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "a4531edd-38c5-442d-853e-9e19ea0a7dff",
      "metadata": {
        "id": "a4531edd-38c5-442d-853e-9e19ea0a7dff"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def remove_URL(text):\n",
        "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
        "    return url.sub(r\"\", text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "b4f07c0e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Before:\n",
            " We always try to bring the heavy. #metal #RT http://t.co/YAo1e0xngw\n",
            "Text After:\n",
            " We always try to bring the heavy. #metal #RT \n"
          ]
        }
      ],
      "source": [
        "print(\"Text Before:\\n\", df_train.text[32])\n",
        "print(\"Text After:\\n\",remove_URL(df_train.text[32]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "7547b478",
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_train = pd.DataFrame({'text' : ['Im going to school', 'How are you', \"Im Ahmed\"]})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "154335b4",
      "metadata": {},
      "source": [
        "#### Removing punctuations from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "60d5f3f7-e72a-4a8a-ae11-a68a35c09fa9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "60d5f3f7-e72a-4a8a-ae11-a68a35c09fa9",
        "outputId": "8d3c2aed-bbf7-472a-a373-c3115de65e79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def remove_punct(text):\n",
        "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "3f04f86b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Before:\n",
            " All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n",
            "Text After:\n",
            " All residents asked to shelter in place are being notified by officers No other evacuation or shelter in place orders are expected\n"
          ]
        }
      ],
      "source": [
        "print(\"Text Before:\\n\", df_train.text[2])\n",
        "print(\"Text After:\\n\",remove_punct(df_train.text[2]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "13b03b0c",
      "metadata": {},
      "source": [
        "#### Removing stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "012fc189-ea1d-4b28-9c1b-08b0a0b49df1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "012fc189-ea1d-4b28-9c1b-08b0a0b49df1",
        "outputId": "d7988644-1727-4721-9499-4e3701ee43d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
            "[nltk_data]     getaddrinfo failed>\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "2dba0421",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Stop words example\n",
        "stopwords.words(\"english\")[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "74ccbbf9-0991-4b83-9be8-84fb5394a817",
      "metadata": {
        "id": "74ccbbf9-0991-4b83-9be8-84fb5394a817"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(text):\n",
        "    stop = set(stopwords.words(\"english\"))\n",
        "    \n",
        "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
        "    return \" \".join(filtered_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "d70ce792",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Before:\n",
            " All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n",
            "Text After:\n",
            " residents asked 'shelter place' notified officers. evacuation shelter place orders expected\n"
          ]
        }
      ],
      "source": [
        "print(\"Text Before:\\n\", df_train.text[2])\n",
        "print(\"Text After:\\n\",remove_stopwords(df_train.text[2]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7814d168-ca43-4c1e-97f2-3267d051296a",
      "metadata": {
        "id": "7814d168-ca43-4c1e-97f2-3267d051296a"
      },
      "source": [
        "#### Now let's apply our cleaning methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "3753649e-abde-481f-865d-f7c528c6a88c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3753649e-abde-481f-865d-f7c528c6a88c",
        "outputId": "4d336960-2155-4b4f-f282-b911342b1266"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0            deeds reason earthquake may allah forgive us\n",
              "1                   forest fire near la ronge sask canada\n",
              "2       residents asked shelter place notified officer...\n",
              "3       13000 people receive wildfires evacuation orde...\n",
              "4       got sent photo ruby alaska smoke wildfires pou...\n",
              "                              ...                        \n",
              "7608    two giant cranes holding bridge collapse nearb...\n",
              "7609    ariaahrary thetawniest control wild fires cali...\n",
              "7610                      m194 0104 utc5km volcano hawaii\n",
              "7611    police investigating ebike collided car little...\n",
              "7612    latest homes razed northern california wildfir...\n",
              "Name: text, Length: 7613, dtype: object"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train[\"text\"] = df_train.text.map(remove_URL) # map(lambda x: remove_URL(x))\n",
        "df_train[\"text\"] = df_train.text.map(remove_punct)\n",
        "df_train[\"text\"] = df_train.text.map(remove_stopwords)\n",
        "df_train[\"text\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "02628923",
      "metadata": {},
      "source": [
        "-------------"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "824eb9bb",
      "metadata": {},
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "3cb98998",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
            "[nltk_data]     getaddrinfo failed>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "8373c67b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    [deeds, reason, earthquake, may, allah, forgiv...\n",
              "1        [forest, fire, near, la, ronge, sask, canada]\n",
              "2    [residents, asked, shelter, place, notified, o...\n",
              "3    [13000, people, receive, wildfires, evacuation...\n",
              "4    [got, sent, photo, ruby, alaska, smoke, wildfi...\n",
              "Name: text, dtype: object"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train[\"text\"] = df_train.text.map(nltk.tokenize.word_tokenize)\n",
        "df_train.text.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b108fc86",
      "metadata": {},
      "source": [
        "### Lemmatizing the tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "ee4657d8",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
            "[nltk_data]     getaddrinfo failed>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "8f29e51a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def Lemmatize(sentence_tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    result_sentence = []\n",
        "    for token in sentence_tokens:\n",
        "        result_sentence.append(lemmatizer.lemmatize(token))\n",
        "    return result_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "fe49af82",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    [deed, reason, earthquake, may, allah, forgive...\n",
              "1        [forest, fire, near, la, ronge, sask, canada]\n",
              "2    [resident, asked, shelter, place, notified, of...\n",
              "3    [13000, people, receive, wildfire, evacuation,...\n",
              "4    [got, sent, photo, ruby, alaska, smoke, wildfi...\n",
              "Name: text, dtype: object"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train[\"text\"] = df_train.text.map(Lemmatize)\n",
        "df_train.text.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "78b5e877",
      "metadata": {},
      "source": [
        "## Calculate Probabilty with N-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "f4e69eaa",
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.util import ngrams"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f0737e22",
      "metadata": {},
      "source": [
        "### Adding start and end tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "47c04eb6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    [<S>, deed, reason, earthquake, may, allah, fo...\n",
              "1    [<S>, forest, fire, near, la, ronge, sask, can...\n",
              "2    [<S>, resident, asked, shelter, place, notifie...\n",
              "3    [<S>, 13000, people, receive, wildfire, evacua...\n",
              "4    [<S>, got, sent, photo, ruby, alaska, smoke, w...\n",
              "Name: text, dtype: object"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train['text'] = df_train.text.map(lambda x: [\"<S>\"] + x + [\"<E>\"])\n",
        "df_train.text.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2b798df0",
      "metadata": {},
      "source": [
        "#### Calculate tokens frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "0bd4fbf0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flatten the token as putting them all in single list\n",
        "def flatten_tokens(df):\n",
        "    tokens = [token for item in df for token in item]      \n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "d4d347a1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('<S>', 7613),\n",
              " ('<E>', 7613),\n",
              " ('fire', 350),\n",
              " ('like', 347),\n",
              " ('im', 299),\n",
              " ('amp', 298),\n",
              " ('get', 255),\n",
              " ('u', 246),\n",
              " ('new', 224),\n",
              " ('via', 220)]"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = flatten_tokens(df_train.text)\n",
        "tokens_fd = nltk.FreqDist(tokens)\n",
        "tokens_fd.most_common(10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "edce08c8",
      "metadata": {},
      "source": [
        "#### Get Bigrams and calculate their frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "5881d8db",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    [(<S>, deed), (deed, reason), (reason, earthqu...\n",
              "1    [(<S>, forest), (forest, fire), (fire, near), ...\n",
              "2    [(<S>, resident), (resident, asked), (asked, s...\n",
              "3    [(<S>, 13000), (13000, people), (people, recei...\n",
              "4    [(<S>, got), (got, sent), (sent, photo), (phot...\n",
              "Name: text, dtype: object"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigrams = df_train.text.apply(lambda x:list(ngrams(x, 2)))\n",
        "bigrams[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "0f018111",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flatten the bigrams as putting them all in single list\n",
        "def flatten_bigrams(df):\n",
        "    bigrams = [bigram for item in df for bigram in item]\n",
        "    return bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "b86fd4da",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "79384"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "flattened = flatten_bigrams(bigrams)\n",
        "len(flatten_bigrams(bigrams))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "3d17fcfa",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(('<S>', 'new'), 74),\n",
              " (('<S>', 'im'), 70),\n",
              " (('suicide', 'bomber'), 60),\n",
              " (('burning', 'building'), 58),\n",
              " (('fire', '<E>'), 58),\n",
              " (('news', '<E>'), 53),\n",
              " (('\\x89û', '<E>'), 50),\n",
              " (('look', 'like'), 49),\n",
              " (('body', 'bag'), 48),\n",
              " (('<S>', 'rt'), 47)]"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fd_bi = nltk.FreqDist(flattened)\n",
        "fd_bi.most_common(10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cf0cb890",
      "metadata": {},
      "source": [
        "### Estimating the probabiltiy of a word sequence\n",
        "P(x1, x2, ..., xn) = P(x1)P(x2|x1)...P(xn|x1,...xn-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "e4d7c623",
      "metadata": {},
      "outputs": [],
      "source": [
        "def prob_calc(sentence):\n",
        "    probs = []\n",
        "    for bigram in sentence:\n",
        "        # Probabilty of current bigram = frequency(bigram) / (frequency(first token of bigram) == number of words prefix to second token of bigram)\n",
        "        prob = fd_bi[bigram] / tokens_fd[bigram[0]] \n",
        "        probs.append(prob)\n",
        "        print(f\"P({bigram[0]} | {bigram[1]}) = {prob:.3}\")\n",
        "    print(\"=\"*40)\n",
        "    \n",
        "    res = probs[0] \n",
        "    sen = \" \".join([word[0] for word in sentence])\n",
        "    print(f\"P({sen}) = ({probs[0]:.4})\",end=\"\")\n",
        "    for prob in probs[1:]:\n",
        "        res *= prob\n",
        "        print(f\" * ({prob:.4})\", end= \"\")\n",
        "    print(f\" = {res:.2}\")\n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9cc9bc99",
      "metadata": {},
      "source": [
        "### Print the likely hood of the first 10 senteces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "5cd4c2c3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P(<S> | deed) = 0.000131\n",
            "P(deed | reason) = 0.5\n",
            "P(reason | earthquake) = 0.0323\n",
            "P(earthquake | may) = 0.0189\n",
            "P(may | allah) = 0.0341\n",
            "P(allah | forgive) = 0.111\n",
            "P(forgive | u) = 0.5\n",
            "P(u | <E>) = 0.0813\n",
            "========================================\n",
            "P(<S> deed reason earthquake may allah forgive u) = (0.0001314) * (0.5) * (0.03226) * (0.01887) * (0.03409) * (0.1111) * (0.5) * (0.0813) = 6.2e-12\n",
            "\n",
            "\n",
            "P(<S> | forest) = 0.000657\n",
            "P(forest | fire) = 0.424\n",
            "P(fire | near) = 0.0114\n",
            "P(near | la) = 0.0185\n",
            "P(la | ronge) = 0.0357\n",
            "P(ronge | sask) = 1.0\n",
            "P(sask | canada) = 1.0\n",
            "P(canada | <E>) = 0.231\n",
            "========================================\n",
            "P(<S> forest fire near la ronge sask canada) = (0.0006568) * (0.4242) * (0.01143) * (0.01852) * (0.03571) * (1.0) * (1.0) * (0.2308) = 4.9e-10\n",
            "\n",
            "\n",
            "P(<S> | resident) = 0.000394\n",
            "P(resident | asked) = 0.125\n",
            "P(asked | shelter) = 0.111\n",
            "P(shelter | place) = 0.333\n",
            "P(place | notified) = 0.0323\n",
            "P(notified | officer) = 1.0\n",
            "P(officer | evacuation) = 0.027\n",
            "P(evacuation | shelter) = 0.0192\n",
            "P(shelter | place) = 0.333\n",
            "P(place | order) = 0.0323\n",
            "P(order | expected) = 0.0286\n",
            "P(expected | <E>) = 0.2\n",
            "========================================\n",
            "P(<S> resident asked shelter place notified officer evacuation shelter place order expected) = (0.0003941) * (0.125) * (0.1111) * (0.3333) * (0.03226) * (1.0) * (0.02703) * (0.01923) * (0.3333) * (0.03226) * (0.02857) * (0.2) = 1.9e-15\n",
            "\n",
            "\n",
            "P(<S> | 13000) = 0.000263\n",
            "P(13000 | people) = 0.25\n",
            "P(people | receive) = 0.00503\n",
            "P(receive | wildfire) = 0.5\n",
            "P(wildfire | evacuation) = 0.012\n",
            "P(evacuation | order) = 0.192\n",
            "P(order | california) = 0.0286\n",
            "P(california | <E>) = 0.0579\n",
            "========================================\n",
            "P(<S> 13000 people receive wildfire evacuation order california) = (0.0002627) * (0.25) * (0.005025) * (0.5) * (0.01205) * (0.1923) * (0.02857) * (0.05785) = 6.3e-13\n",
            "\n",
            "\n",
            "P(<S> | got) = 0.00236\n",
            "P(got | sent) = 0.00813\n",
            "P(sent | photo) = 0.0769\n",
            "P(photo | ruby) = 0.0154\n",
            "P(ruby | alaska) = 1.0\n",
            "P(alaska | smoke) = 0.143\n",
            "P(smoke | wildfire) = 0.0204\n",
            "P(wildfire | pours) = 0.012\n",
            "P(pours | school) = 1.0\n",
            "P(school | <E>) = 0.0571\n",
            "========================================\n",
            "P(<S> got sent photo ruby alaska smoke wildfire pours school) = (0.002364) * (0.00813) * (0.07692) * (0.01538) * (1.0) * (0.1429) * (0.02041) * (0.01205) * (1.0) * (0.05714) = 4.6e-14\n",
            "\n",
            "\n",
            "P(<S> | rockyfire) = 0.000131\n",
            "P(rockyfire | update) = 0.25\n",
            "P(update | california) = 0.0208\n",
            "P(california | hwy) = 0.00826\n",
            "P(hwy | 20) = 0.222\n",
            "P(20 | closed) = 0.0357\n",
            "P(closed | direction) = 0.05\n",
            "P(direction | due) = 0.0667\n",
            "P(due | lake) = 0.0323\n",
            "P(lake | county) = 0.0625\n",
            "P(county | fire) = 0.0526\n",
            "P(fire | cafire) = 0.00286\n",
            "P(cafire | wildfire) = 0.5\n",
            "P(wildfire | <E>) = 0.193\n",
            "========================================\n",
            "P(<S> rockyfire update california hwy 20 closed direction due lake county fire cafire wildfire) = (0.0001314) * (0.25) * (0.02083) * (0.008264) * (0.2222) * (0.03571) * (0.05) * (0.06667) * (0.03226) * (0.0625) * (0.05263) * (0.002857) * (0.5) * (0.1928) = 4.4e-21\n",
            "\n",
            "\n",
            "P(<S> | flood) = 0.00197\n",
            "P(flood | disaster) = 0.0265\n",
            "P(disaster | heavy) = 0.00645\n",
            "P(heavy | rain) = 0.45\n",
            "P(rain | cause) = 0.0192\n",
            "P(cause | flash) = 0.0164\n",
            "P(flash | flooding) = 0.19\n",
            "P(flooding | street) = 0.02\n",
            "P(street | manitou) = 0.0312\n",
            "P(manitou | colorado) = 1.0\n",
            "P(colorado | spring) = 0.0625\n",
            "P(spring | area) = 0.0556\n",
            "P(area | <E>) = 0.208\n",
            "========================================\n",
            "P(<S> flood disaster heavy rain cause flash flooding street manitou colorado spring area) = (0.00197) * (0.02655) * (0.006452) * (0.45) * (0.01923) * (0.01639) * (0.1905) * (0.02) * (0.03125) * (1.0) * (0.0625) * (0.05556) * (0.2083) = 4.1e-18\n",
            "\n",
            "\n",
            "P(<S> | im) = 0.00919\n",
            "P(im | top) = 0.00334\n",
            "P(top | hill) = 0.0179\n",
            "P(hill | see) = 0.0833\n",
            "P(see | fire) = 0.0268\n",
            "P(fire | wood) = 0.00286\n",
            "P(wood | <E>) = 0.167\n",
            "========================================\n",
            "P(<S> im top hill see fire wood) = (0.009195) * (0.003344) * (0.01786) * (0.08333) * (0.02679) * (0.002857) * (0.1667) = 5.8e-13\n",
            "\n",
            "\n",
            "P(<S> | there) = 0.00171\n",
            "P(there | emergency) = 0.0222\n",
            "P(emergency | evacuation) = 0.0127\n",
            "P(evacuation | happening) = 0.0192\n",
            "P(happening | building) = 0.0769\n",
            "P(building | across) = 0.00714\n",
            "P(across | street) = 0.05\n",
            "P(street | <E>) = 0.25\n",
            "========================================\n",
            "P(<S> there emergency evacuation happening building across street) = (0.001708) * (0.02222) * (0.01266) * (0.01923) * (0.07692) * (0.007143) * (0.05) * (0.25) = 6.3e-14\n",
            "\n",
            "\n",
            "P(<S> | im) = 0.00919\n",
            "P(im | afraid) = 0.00334\n",
            "P(afraid | tornado) = 0.2\n",
            "P(tornado | coming) = 0.0263\n",
            "P(coming | area) = 0.0192\n",
            "P(area | <E>) = 0.208\n",
            "========================================\n",
            "P(<S> im afraid tornado coming area) = (0.009195) * (0.003344) * (0.2) * (0.02632) * (0.01923) * (0.2083) = 6.5e-10\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for bigram in bigrams[0:10]:\n",
        "    prob_calc(bigram)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "547e69c8",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "1ad7b6933dbeb3066655e7b4b1019487631d0edeeb6c28cdb2f62ca1713ab7c6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
